Configurations:
Namespace(MP_dim=10, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=300, base_dir=u'./snli', batch_size=60, char_emb_dim=20, char_lstm_dim=100, config_file='quora.sample.config', context_layer_num=1, context_lstm_dim=100, dev_path=u'/users/ud2017/hoavt/nli/snli_1.0/snli_1.0_dev.tsv', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=1, lambda_l2=0.0, learning_rate=0.001, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, model_dir=u'../models', optimize_type=u'adam', suffix=u'quora', test_path=u'/users/ud2017/hoavt/nli/snli_1.0/snli_1.0_test.tsv', train_path=u'/users/ud2017/hoavt/nli/snli_1.0/snli_1.0_train.tsv', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_attentive_match=False, wo_char=False, wo_full_match=False, wo_left_match=False, wo_max_attentive_match=False, wo_maxpool_match=False, wo_right_match=False, word_level_MP_dim=-1, word_vec_path=u'../pre-wordvec/glove.840B.300d.txt')
is -0.084961 0.502 0.0023823 -0.16755 0.30721 -0.23762 0.16069 -0.36786 -0.058347 2.499 -0.0023647 0.010732 -0.30422 0.084579 -0.040299 -0.41562 -0.024494 1.4691 -0.052932 -0.074413 -0.39244 -0.32535 -0.22333 0.0056823 0.35675 0.19445 0.056762 -0.045502 -0.28105 -0.058896 -0.098626 0.092177 0.33172 -0.039967 -0.11766 0.048373 -0.062241 -0.10413 0.00099263 -0.48925 0.34786 0.32724 0.13882 -0.19917 0.12995 0.060549 -0.23714 -0.51295 -0.37396 0.12902 0.055797 0.33444 -0.18025 -0.03474 0.28323 -0.095301 0.21143 -0.076149 0.15069 -0.17441 -0.0074768 -0.078287 -0.12751 0.22545 0.035101 -0.61015 -0.26812 0.061632 -0.30503 -0.13405 -0.44271 -0.1772 0.17663 -0.3121 -0.25722 -0.024858 0.072504 -0.079759 -0.19214 0.59602 0.1288 -0.074629 -0.15812 0.36394 0.23055 -0.42175 -0.090651 -0.30085 0.1794 -0.29786 -0.10642 0.47239 -0.13837 -0.10161 0.080134 0.040715 -0.36976 -0.037066 0.10436 0.17904 0.15702 -0.07467 -0.29431 0.12829 0.055211 -0.43906 -0.068231 0.097107 -0.28209 -0.086528 -0.24204 0.028734 -0.21509 0.027152 -0.17996 0.19317 -0.27929 0.29415 -0.10965 -0.10432 -0.5217 -0.046789 0.13743 -0.15518 -0.10359 0.18853 -0.12684 -0.67278 0.034483 -0.22937 -0.098073 -0.070157 0.084374 0.26594 0.23104 -0.29251 -0.087209 -0.23342 0.063759 -0.13556 -0.84046 0.24681 0.30498 0.35438 0.14137 -0.3672 0.23321 -0.15497 0.48364 0.014711 -0.24176 0.037589 0.19829 -0.069403 -0.0017362 0.041694 -0.34193 -0.20034 -0.45581 -0.12504 0.13954 0.032275 -0.005213 0.045422 -0.0026574 -0.26266 0.064168 -0.14231 0.00035709 -0.23253 0.027615 -0.074282 0.18671 -0.12994 -0.43731 0.1455 0.044838 -0.19022 -0.15401 0.14188 0.098269 -0.04293 -0.27478 -0.33224 -0.32167 -0.10509 -0.19816 -0.065097 -0.091251 0.19528 -0.33297 -0.15504 -0.47688 0.31985 0.19886 0.11501 0.055757 -0.054307 0.28851 0.27982 0.01396 -0.0012891 -0.23128 0.075396 0.043587 -0.13937 -0.062935 0.12568 0.095235 -0.085203 -0.24241 -0.048771 0.095937 -0.22347 0.23503 0.31517 -0.0149 0.21739 -0.19431 -0.23255 -0.22961 0.048297 0.1605 -0.0161 0.04277 -0.32367 0.1468 0.24551 0.075506 -0.039703 -0.10321 0.16194 0.27132 0.046348 -0.092743 -0.14929 0.27378 -0.36958 -0.4153 0.18402 -0.049775 0.06967 0.13447 0.17788 -0.047586 -0.36491 -0.13733 -0.48119 0.24681 -0.089842 0.037939 -0.18284 0.47012 -0.099584 -0.18365 -0.071821 0.41607 -0.18581 0.184 -0.029028 0.41228 0.022856 0.050915 -0.11911 0.081231 0.13845 0.046595 -0.043974 0.63601 0.0037101 0.093937 -0.093442 -0.47606 -0.26427 -0.023044 -0.058241 0.1144 -0.051702 0.35225 0.25341 0.57256 0.22867 0.0085401 -0.062531 -0.032118 -0.15647 -0.084344 0.076667 0.34515 -0.19452 0.087003 -0.078201 -0.069673 -0.16993 0.23598 0.2755 -0.06718 -0.21511 -0.26304 -0.0060173
vocabulary size:  500002
word dim:  300
word dim from file:  (299,)
word dim from file:  (299,)
word dim from file:  (299,)
Collect words, chars and labels ...
Number of words: 36617
Number of labels: 3
Number of chars: 60
word_vocab shape is (500003, 300)
tag_vocab shape is (4, 2)
Build SentenceMatchDataStream ... 
Number of instances in trainDataStream: 549367
Number of instances in devDataStream: 9842
Number of instances in testDataStream: 9824
Number of batches in trainDataStream: 9157
Number of batches in devDataStream: 165
Number of batches in testDataStream: 164
/users/ud2017/hoavt/hoa_env/snli/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From SentenceMatchTrainer.py:235 in main.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.
Start the training loop.
0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 8400 8500 8600 8700 8800 8900 9000 9100 
Step 9156: loss = 5060.14 (10202.864 sec)
Validation Data Eval:
Current accuracy is 84.48
9200 9300 9400 9500 9600 9700 9800 9900 10000 10100 10200 10300 10400 10500 10600 10700 10800 10900 11000 11100 11200 11300 11400 11500 11600 11700 11800 11900 12000 12100 12200 12300 12400 12500 12600 12700 12800 12900 13000 13100 13200 13300 13400 13500 13600 13700 13800 13900 14000 14100 14200 14300 14400 14500 14600 14700 14800 14900 15000 15100 15200 15300 15400 15500 15600 15700 15800 15900 16000 16100 16200 16300 16400 16500 16600 16700 16800 16900 17000 17100 17200 17300 17400 17500 17600 17700 17800 17900 18000 18100 18200 18300 
Step 18313: loss = 3990.75 (10368.138 sec)
Validation Data Eval:
Current accuracy is 85.44
18400 18500 18600 18700 18800 18900 19000 19100 19200 19300 19400 19500 19600 19700 19800 19900 20000 20100 20200 20300 20400 20500 20600 20700 20800 20900 21000 21100 21200 21300 21400 21500 21600 21700 21800 21900 22000 22100 22200 22300 22400 22500 22600 22700 22800 22900 23000 23100 23200 23300 23400 23500 23600 23700 23800 23900 24000 24100 24200 24300 24400 24500 24600 24700 24800 24900 25000 25100 25200 25300 25400 25500 25600 25700 25800 25900 26000 26100 26200 26300 26400 26500 26600 26700 26800 26900 27000 27100 27200 27300 27400 
Step 27470: loss = 3660.69 (10376.983 sec)
Validation Data Eval:
Current accuracy is 85.69
27500 27600 27700 27800 27900 28000 28100 28200 28300 28400 28500 28600 28700 28800 28900 29000 29100 29200 29300 29400 29500 29600 29700 29800 29900 30000 30100 30200 30300 30400 30500 30600 30700 30800 30900 31000 31100 31200 31300 31400 31500 31600 31700 31800 31900 32000 32100 32200 32300 32400 32500 32600 32700 32800 32900 33000 33100 33200 33300 33400 33500 33600 33700 33800 33900 34000 34100 34200 34300 34400 34500 34600 34700 34800 34900 35000 35100 35200 35300 35400 35500 35600 35700 35800 35900 36000 36100 36200 36300 36400 36500 36600 
Step 36627: loss = 3447.67 (10362.269 sec)
Validation Data Eval:
Current accuracy is 86.68
36700 36800 36900 37000 37100 37200 37300 37400 37500 37600 37700 37800 37900 38000 38100 38200 38300 38400 38500 38600 38700 38800 38900 39000 39100 39200 39300 39400 39500 39600 39700 39800 39900 40000 40100 40200 40300 40400 40500 40600 40700 40800 40900 41000 41100 41200 41300 41400 41500 41600 41700 41800 41900 42000 42100 42200 42300 42400 42500 42600 42700 42800 42900 43000 43100 43200 43300 43400 43500 43600 43700 43800 43900 44000 44100 44200 44300 44400 44500 44600 44700 44800 44900 45000 45100 45200 45300 45400 45500 45600 45700 
Step 45784: loss = 3288.08 (10356.051 sec)
Validation Data Eval:
Current accuracy is 87.06
45800 45900 46000 46100 46200 46300 46400 46500 46600 46700 46800 46900 47000 47100 47200 47300 47400 47500 47600 47700 47800 47900 48000 48100 48200 48300 48400 48500 48600 48700 48800 48900 49000 49100 49200 49300 49400 49500 49600 49700 49800 49900 50000 50100 50200 50300 50400 50500 50600 50700 50800 50900 51000 51100 51200 51300 51400 51500 51600 51700 51800 51900 52000 52100 52200 52300 52400 52500 52600 52700 52800 52900 53000 53100 53200 53300 53400 53500 53600 53700 53800 53900 54000 54100 54200 54300 54400 54500 54600 54700 54800 54900 
Step 54941: loss = 3167.92 (10352.213 sec)
Validation Data Eval:
Current accuracy is 87.17
55000 55100 55200 55300 55400 55500 55600 55700 55800 55900 56000 56100 56200 56300 56400 56500 56600 56700 56800 56900 57000 57100 57200 57300 57400 57500 57600 57700 57800 57900 58000 58100 58200 58300 58400 58500 58600 58700 58800 58900 59000 59100 59200 59300 59400 59500 59600 59700 59800 59900 60000 60100 60200 60300 60400 60500 60600 60700 60800 60900 61000 61100 61200 61300 61400 61500 61600 61700 61800 61900 62000 62100 62200 62300 62400 62500 62600 62700 62800 62900 63000 63100 63200 63300 63400 63500 63600 63700 63800 63900 64000 
Step 64098: loss = 3061.75 (10342.010 sec)
Validation Data Eval:
Current accuracy is 87.17
64100 64200 64300 64400 64500 64600 64700 64800 64900 65000 65100 65200 65300 65400 65500 65600 65700 65800 65900 66000 66100 66200 66300 66400 66500 66600 66700 66800 66900 67000 67100 67200 67300 67400 67500 67600 67700 67800 67900 68000 68100 68200 68300 68400 68500 68600 68700 68800 68900 69000 69100 69200 69300 69400 69500 69600 69700 69800 69900 70000 70100 70200 70300 70400 70500 70600 70700 70800 70900 71000 71100 71200 71300 71400 71500 71600 71700 71800 71900 72000 72100 72200 72300 72400 72500 72600 72700 72800 72900 73000 73100 73200 
Step 73255: loss = 2975.72 (10322.467 sec)
Validation Data Eval:
Current accuracy is 86.91
73300 73400 73500 73600 73700 73800 73900 74000 74100 74200 74300 74400 74500 74600 74700 74800 74900 75000 75100 75200 75300 75400 75500 75600 75700 75800 75900 76000 76100 76200 76300 76400 76500 76600 76700 76800 76900 77000 77100 77200 77300 77400 77500 77600 77700 77800 77900 78000 78100 78200 78300 78400 78500 78600 78700 78800 78900 79000 79100 79200 79300 79400 79500 79600 79700 79800 79900 80000 80100 80200 80300 80400 80500 80600 80700 80800 80900 81000 81100 81200 81300 81400 81500 81600 81700 81800 81900 82000 82100 82200 82300 82400 
Step 82412: loss = 2900.12 (10321.011 sec)
Validation Data Eval:
Current accuracy is 87.04
82500 82600 82700 82800 82900 83000 83100 83200 83300 83400 83500 83600 83700 83800 83900 84000 84100 84200 84300 84400 84500 84600 84700 84800 84900 85000 85100 85200 85300 85400 85500 85600 85700 85800 85900 86000 86100 86200 86300 86400 86500 86600 86700 86800 86900 87000 87100 87200 87300 87400 87500 87600 87700 87800 87900 88000 88100 88200 88300 88400 88500 88600 88700 88800 88900 89000 89100 89200 89300 89400 89500 89600 89700 89800 89900 90000 90100 90200 90300 90400 90500 90600 90700 90800 90900 91000 91100 91200 91300 91400 91500 WARNING:tensorflow:From SentenceMatchTrainer.py:330 in main.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Please use tf.global_variables instead.

Step 91569: loss = 2834.71 (10317.602 sec)
Validation Data Eval:
Current accuracy is 86.54
Best accuracy on dev set is 87.17
Decoding on the test set:
Accuracy for test set is 86.52
